{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF4d29nhujCt"
      },
      "source": [
        "IMBD2021 設備感測資料於物理特性值預測: LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsYSnTJwuqRv"
      },
      "source": [
        "匯入雲端硬碟"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFfwTM86uird",
        "outputId": "f5d3c3c0-23a0-4fc6-d750-913bb908636a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "data\t\t     IMBD2021_LSTM.ipynb  pred_result  train.csv\n",
            "imbd2021_LGBM.ipynb  model\t\t  test.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test.csv',\n",
              " 'train.csv',\n",
              " 'data',\n",
              " 'pred_result',\n",
              " 'imbd2021_LGBM.ipynb',\n",
              " 'model',\n",
              " 'IMBD2021_LSTM.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls /content/drive/My\\ Drive/Colab\\ Notebooks/imbd2021\n",
        "\n",
        "import os\n",
        "path='/content/drive/My Drive/Colab Notebooks/imbd2021/'\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b624wv0l6CMW"
      },
      "source": [
        "LSTM程式碼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tCZFGH-uLVf",
        "outputId": "d26d6dae-b331-4635-d992-0a94f1d397c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch is 1 \tTraining Loss: 936.574617\n",
            "Epoch is 2 \tTraining Loss: 923.936351\n",
            "Epoch is 3 \tTraining Loss: 916.332160\n",
            "Epoch is 4 \tTraining Loss: 909.357726\n",
            "Epoch is 5 \tTraining Loss: 903.226175\n",
            "Epoch is 6 \tTraining Loss: 897.563388\n",
            "Epoch is 7 \tTraining Loss: 892.276669\n",
            "Epoch is 8 \tTraining Loss: 887.285160\n",
            "Epoch is 9 \tTraining Loss: 882.013164\n",
            "Epoch is 10 \tTraining Loss: 876.746731\n",
            "Epoch is 11 \tTraining Loss: 871.076442\n",
            "Epoch is 12 \tTraining Loss: 865.126013\n",
            "Epoch is 13 \tTraining Loss: 859.466323\n",
            "Epoch is 14 \tTraining Loss: 853.750046\n",
            "Epoch is 15 \tTraining Loss: 848.396973\n",
            "Epoch is 16 \tTraining Loss: 842.753143\n",
            "Epoch is 17 \tTraining Loss: 837.246592\n",
            "Epoch is 18 \tTraining Loss: 831.751105\n",
            "Epoch is 19 \tTraining Loss: 826.245697\n",
            "Epoch is 20 \tTraining Loss: 820.526854\n",
            "Epoch is 21 \tTraining Loss: 814.837933\n",
            "Epoch is 22 \tTraining Loss: 809.202044\n",
            "Epoch is 23 \tTraining Loss: 802.954269\n",
            "Epoch is 24 \tTraining Loss: 796.166307\n",
            "Epoch is 25 \tTraining Loss: 789.950246\n",
            "Epoch is 26 \tTraining Loss: 783.704592\n",
            "Epoch is 27 \tTraining Loss: 777.298341\n",
            "Epoch is 28 \tTraining Loss: 771.041927\n",
            "Epoch is 29 \tTraining Loss: 764.629986\n",
            "Epoch is 30 \tTraining Loss: 758.173394\n",
            "Epoch is 31 \tTraining Loss: 751.806945\n",
            "Epoch is 32 \tTraining Loss: 745.142939\n",
            "Epoch is 33 \tTraining Loss: 738.760480\n",
            "Epoch is 34 \tTraining Loss: 732.038946\n",
            "Epoch is 35 \tTraining Loss: 725.037899\n",
            "Epoch is 36 \tTraining Loss: 717.537543\n",
            "Epoch is 37 \tTraining Loss: 709.966328\n",
            "Epoch is 38 \tTraining Loss: 702.169421\n",
            "Epoch is 39 \tTraining Loss: 694.455551\n",
            "Epoch is 40 \tTraining Loss: 686.700217\n",
            "Epoch is 41 \tTraining Loss: 678.932393\n",
            "Epoch is 42 \tTraining Loss: 671.233786\n",
            "Epoch is 43 \tTraining Loss: 663.405176\n",
            "Epoch is 44 \tTraining Loss: 655.465494\n",
            "Epoch is 45 \tTraining Loss: 647.863474\n",
            "Epoch is 46 \tTraining Loss: 639.867482\n",
            "Epoch is 47 \tTraining Loss: 632.085161\n",
            "Epoch is 48 \tTraining Loss: 624.279481\n",
            "Epoch is 49 \tTraining Loss: 616.596441\n",
            "Epoch is 50 \tTraining Loss: 608.314112\n",
            "Epoch is 51 \tTraining Loss: 600.454113\n",
            "Epoch is 52 \tTraining Loss: 592.417405\n",
            "Epoch is 53 \tTraining Loss: 584.413992\n",
            "Epoch is 54 \tTraining Loss: 576.287023\n",
            "Epoch is 55 \tTraining Loss: 568.392829\n",
            "Epoch is 56 \tTraining Loss: 559.621829\n",
            "Epoch is 57 \tTraining Loss: 551.347559\n",
            "Epoch is 58 \tTraining Loss: 542.629555\n",
            "Epoch is 59 \tTraining Loss: 532.337928\n",
            "Epoch is 60 \tTraining Loss: 522.810143\n",
            "Epoch is 61 \tTraining Loss: 513.792599\n",
            "Epoch is 62 \tTraining Loss: 504.395150\n",
            "Epoch is 63 \tTraining Loss: 495.550848\n",
            "Epoch is 64 \tTraining Loss: 486.569989\n",
            "Epoch is 65 \tTraining Loss: 477.541989\n",
            "Epoch is 66 \tTraining Loss: 468.254940\n",
            "Epoch is 67 \tTraining Loss: 458.390505\n",
            "Epoch is 68 \tTraining Loss: 449.308725\n",
            "Epoch is 69 \tTraining Loss: 440.187444\n",
            "Epoch is 70 \tTraining Loss: 430.277135\n",
            "Epoch is 71 \tTraining Loss: 419.357108\n",
            "Epoch is 72 \tTraining Loss: 408.740853\n",
            "Epoch is 73 \tTraining Loss: 399.225073\n",
            "Epoch is 74 \tTraining Loss: 389.599485\n",
            "Epoch is 75 \tTraining Loss: 380.570567\n",
            "Epoch is 76 \tTraining Loss: 370.982751\n",
            "Epoch is 77 \tTraining Loss: 362.003937\n",
            "Epoch is 78 \tTraining Loss: 351.404654\n",
            "Epoch is 79 \tTraining Loss: 341.588256\n",
            "Epoch is 80 \tTraining Loss: 332.689411\n",
            "Epoch is 81 \tTraining Loss: 323.959104\n",
            "Epoch is 82 \tTraining Loss: 315.032374\n",
            "Epoch is 83 \tTraining Loss: 306.556996\n",
            "Epoch is 84 \tTraining Loss: 298.094807\n",
            "Epoch is 85 \tTraining Loss: 289.799515\n",
            "Epoch is 86 \tTraining Loss: 281.480155\n",
            "Epoch is 87 \tTraining Loss: 273.313999\n",
            "Epoch is 88 \tTraining Loss: 265.458470\n",
            "Epoch is 89 \tTraining Loss: 257.400377\n",
            "Epoch is 90 \tTraining Loss: 249.583461\n",
            "Epoch is 91 \tTraining Loss: 241.878327\n",
            "Epoch is 92 \tTraining Loss: 234.194896\n",
            "Epoch is 93 \tTraining Loss: 226.779586\n",
            "Epoch is 94 \tTraining Loss: 219.669966\n",
            "Epoch is 95 \tTraining Loss: 212.250973\n",
            "Epoch is 96 \tTraining Loss: 204.984375\n",
            "Epoch is 97 \tTraining Loss: 198.171204\n",
            "Epoch is 98 \tTraining Loss: 191.053088\n",
            "Epoch is 99 \tTraining Loss: 184.484082\n",
            "Epoch is 100 \tTraining Loss: 177.839568\n",
            "Epoch is 101 \tTraining Loss: 171.497282\n",
            "Epoch is 102 \tTraining Loss: 164.896602\n",
            "Epoch is 103 \tTraining Loss: 159.031285\n",
            "Epoch is 104 \tTraining Loss: 152.641162\n",
            "Epoch is 105 \tTraining Loss: 146.645893\n",
            "Epoch is 106 \tTraining Loss: 141.149889\n",
            "Epoch is 107 \tTraining Loss: 135.577145\n",
            "Epoch is 108 \tTraining Loss: 130.069524\n",
            "Epoch is 109 \tTraining Loss: 124.852183\n",
            "Epoch is 110 \tTraining Loss: 119.741431\n",
            "Epoch is 111 \tTraining Loss: 114.656458\n",
            "Epoch is 112 \tTraining Loss: 109.629145\n",
            "Epoch is 113 \tTraining Loss: 104.837993\n",
            "Epoch is 114 \tTraining Loss: 100.645638\n",
            "Epoch is 115 \tTraining Loss: 96.099888\n",
            "Epoch is 116 \tTraining Loss: 91.944846\n",
            "Epoch is 117 \tTraining Loss: 87.592247\n",
            "Epoch is 118 \tTraining Loss: 83.481050\n",
            "Epoch is 119 \tTraining Loss: 78.772631\n",
            "Epoch is 120 \tTraining Loss: 74.641905\n",
            "Epoch is 121 \tTraining Loss: 70.408458\n",
            "Epoch is 122 \tTraining Loss: 66.123711\n",
            "Epoch is 123 \tTraining Loss: 62.477390\n",
            "Epoch is 124 \tTraining Loss: 58.896692\n",
            "Epoch is 125 \tTraining Loss: 55.820661\n",
            "Epoch is 126 \tTraining Loss: 52.652168\n",
            "Epoch is 127 \tTraining Loss: 49.909692\n",
            "Epoch is 128 \tTraining Loss: 46.908779\n",
            "Epoch is 129 \tTraining Loss: 44.121742\n",
            "Epoch is 130 \tTraining Loss: 41.551033\n",
            "Epoch is 131 \tTraining Loss: 39.074032\n",
            "Epoch is 132 \tTraining Loss: 37.099236\n",
            "Epoch is 133 \tTraining Loss: 34.969787\n",
            "Epoch is 134 \tTraining Loss: 33.173256\n",
            "Epoch is 135 \tTraining Loss: 30.977518\n",
            "Epoch is 136 \tTraining Loss: 29.361296\n",
            "Epoch is 137 \tTraining Loss: 27.621174\n",
            "Epoch is 138 \tTraining Loss: 26.047952\n",
            "Epoch is 139 \tTraining Loss: 24.718189\n",
            "Epoch is 140 \tTraining Loss: 23.477396\n",
            "Epoch is 141 \tTraining Loss: 22.298335\n",
            "Epoch is 142 \tTraining Loss: 21.066195\n",
            "Epoch is 143 \tTraining Loss: 20.177407\n",
            "Epoch is 144 \tTraining Loss: 19.232497\n",
            "Epoch is 145 \tTraining Loss: 18.405510\n",
            "Epoch is 146 \tTraining Loss: 17.925552\n",
            "Epoch is 147 \tTraining Loss: 16.832023\n",
            "Epoch is 148 \tTraining Loss: 16.400610\n",
            "Epoch is 149 \tTraining Loss: 15.670320\n",
            "Epoch is 150 \tTraining Loss: 14.135837\n",
            "Epoch is 151 \tTraining Loss: 11.879546\n",
            "Epoch is 152 \tTraining Loss: 11.264949\n",
            "Epoch is 153 \tTraining Loss: 10.873030\n",
            "Epoch is 154 \tTraining Loss: 10.163854\n",
            "Epoch is 155 \tTraining Loss: 9.667446\n",
            "Epoch is 156 \tTraining Loss: 9.285277\n",
            "Epoch is 157 \tTraining Loss: 9.388186\n",
            "Epoch is 158 \tTraining Loss: 9.183786\n",
            "Epoch is 159 \tTraining Loss: 8.894746\n",
            "Epoch is 160 \tTraining Loss: 8.737266\n",
            "Epoch is 161 \tTraining Loss: 8.566482\n",
            "Epoch is 162 \tTraining Loss: 8.267428\n",
            "Epoch is 163 \tTraining Loss: 8.097885\n",
            "Epoch is 164 \tTraining Loss: 8.136144\n",
            "Epoch is 165 \tTraining Loss: 8.057130\n",
            "Epoch is 166 \tTraining Loss: 7.862927\n",
            "Epoch is 167 \tTraining Loss: 7.810559\n",
            "Epoch is 168 \tTraining Loss: 7.946163\n",
            "Epoch is 169 \tTraining Loss: 7.774517\n",
            "Epoch is 170 \tTraining Loss: 7.778215\n",
            "Epoch is 171 \tTraining Loss: 7.595254\n",
            "Epoch is 172 \tTraining Loss: 7.512142\n",
            "Epoch is 173 \tTraining Loss: 7.517221\n",
            "Epoch is 174 \tTraining Loss: 7.481399\n",
            "Epoch is 175 \tTraining Loss: 7.476227\n",
            "Epoch is 176 \tTraining Loss: 7.235188\n",
            "Epoch is 177 \tTraining Loss: 7.162796\n",
            "Epoch is 178 \tTraining Loss: 7.011323\n",
            "Epoch is 179 \tTraining Loss: 7.076310\n",
            "Epoch is 180 \tTraining Loss: 7.198988\n",
            "Epoch is 181 \tTraining Loss: 6.922391\n",
            "Epoch is 182 \tTraining Loss: 7.034129\n",
            "Epoch is 183 \tTraining Loss: 7.119043\n",
            "Epoch is 184 \tTraining Loss: 6.910006\n",
            "Epoch is 185 \tTraining Loss: 7.089332\n",
            "Epoch is 186 \tTraining Loss: 6.761708\n",
            "Epoch is 187 \tTraining Loss: 6.715793\n",
            "Epoch is 188 \tTraining Loss: 6.828411\n",
            "Epoch is 189 \tTraining Loss: 6.794170\n",
            "Epoch is 190 \tTraining Loss: 7.005410\n",
            "Epoch is 191 \tTraining Loss: 6.540126\n",
            "Epoch is 192 \tTraining Loss: 6.629763\n",
            "Epoch is 193 \tTraining Loss: 6.704983\n",
            "Epoch is 194 \tTraining Loss: 6.572477\n",
            "Epoch is 195 \tTraining Loss: 6.741838\n",
            "Epoch is 196 \tTraining Loss: 6.624523\n",
            "Epoch is 197 \tTraining Loss: 6.665906\n",
            "Epoch is 198 \tTraining Loss: 6.701941\n",
            "Epoch is 199 \tTraining Loss: 6.482389\n",
            "Epoch is 200 \tTraining Loss: 6.547815\n",
            "Epoch is 201 \tTraining Loss: 6.440397\n",
            "Epoch is 202 \tTraining Loss: 6.307945\n",
            "Epoch is 203 \tTraining Loss: 6.488800\n",
            "Epoch is 204 \tTraining Loss: 6.690432\n",
            "Epoch is 205 \tTraining Loss: 6.474119\n",
            "Epoch is 206 \tTraining Loss: 6.302107\n",
            "Epoch is 207 \tTraining Loss: 6.450754\n",
            "Epoch is 208 \tTraining Loss: 6.609078\n",
            "Epoch is 209 \tTraining Loss: 6.163123\n",
            "Epoch is 210 \tTraining Loss: 6.604701\n",
            "Epoch is 211 \tTraining Loss: 6.271090\n",
            "Epoch is 212 \tTraining Loss: 6.234396\n",
            "Epoch is 213 \tTraining Loss: 6.385066\n",
            "Epoch is 214 \tTraining Loss: 6.379774\n",
            "Epoch is 215 \tTraining Loss: 6.188468\n",
            "Epoch is 216 \tTraining Loss: 6.159087\n",
            "Epoch is 217 \tTraining Loss: 6.102270\n",
            "Epoch is 218 \tTraining Loss: 6.105139\n",
            "Epoch is 219 \tTraining Loss: 6.133797\n",
            "Epoch is 220 \tTraining Loss: 6.029104\n",
            "Epoch is 221 \tTraining Loss: 6.044443\n",
            "Epoch is 222 \tTraining Loss: 6.349192\n",
            "Epoch is 223 \tTraining Loss: 6.089327\n",
            "Epoch is 224 \tTraining Loss: 6.176818\n",
            "Epoch is 225 \tTraining Loss: 5.989409\n",
            "Epoch is 226 \tTraining Loss: 6.288340\n",
            "Epoch is 227 \tTraining Loss: 6.087095\n",
            "Epoch is 228 \tTraining Loss: 6.051742\n",
            "Epoch is 229 \tTraining Loss: 6.009489\n",
            "Epoch is 230 \tTraining Loss: 5.810405\n",
            "Epoch is 231 \tTraining Loss: 5.957133\n",
            "Epoch is 232 \tTraining Loss: 5.883178\n",
            "Epoch is 233 \tTraining Loss: 5.896377\n",
            "Epoch is 234 \tTraining Loss: 5.985025\n",
            "Epoch is 235 \tTraining Loss: 6.069709\n",
            "Epoch is 236 \tTraining Loss: 5.818819\n",
            "Epoch is 237 \tTraining Loss: 5.816743\n",
            "Epoch is 238 \tTraining Loss: 5.959151\n",
            "Epoch is 239 \tTraining Loss: 5.823962\n",
            "Epoch is 240 \tTraining Loss: 5.855088\n",
            "Epoch is 241 \tTraining Loss: 5.854278\n",
            "Epoch is 242 \tTraining Loss: 5.717852\n",
            "Epoch is 243 \tTraining Loss: 5.869598\n",
            "Epoch is 244 \tTraining Loss: 5.624438\n",
            "Epoch is 245 \tTraining Loss: 5.911375\n",
            "Epoch is 246 \tTraining Loss: 5.996697\n",
            "Epoch is 247 \tTraining Loss: 5.940615\n",
            "Epoch is 248 \tTraining Loss: 5.883082\n",
            "Epoch is 249 \tTraining Loss: 5.816528\n",
            "Epoch is 250 \tTraining Loss: 5.781580\n",
            "Epoch is 251 \tTraining Loss: 5.768175\n",
            "Epoch is 252 \tTraining Loss: 5.765101\n",
            "Epoch is 253 \tTraining Loss: 5.481497\n",
            "Epoch is 254 \tTraining Loss: 5.735816\n",
            "Epoch is 255 \tTraining Loss: 5.579153\n",
            "Epoch is 256 \tTraining Loss: 5.702689\n",
            "Epoch is 257 \tTraining Loss: 5.667066\n",
            "Epoch is 258 \tTraining Loss: 5.683740\n",
            "Epoch is 259 \tTraining Loss: 5.574779\n",
            "Epoch is 260 \tTraining Loss: 5.466357\n",
            "Epoch is 261 \tTraining Loss: 5.513115\n",
            "Epoch is 262 \tTraining Loss: 5.577955\n",
            "Epoch is 263 \tTraining Loss: 5.573567\n",
            "Epoch is 264 \tTraining Loss: 5.543467\n",
            "Epoch is 265 \tTraining Loss: 5.494206\n",
            "Epoch is 266 \tTraining Loss: 5.528604\n",
            "Epoch is 267 \tTraining Loss: 5.323877\n",
            "Epoch is 268 \tTraining Loss: 5.581021\n",
            "Epoch is 269 \tTraining Loss: 5.528333\n",
            "Epoch is 270 \tTraining Loss: 5.461664\n",
            "Epoch is 271 \tTraining Loss: 5.288720\n",
            "Epoch is 272 \tTraining Loss: 5.436155\n",
            "Epoch is 273 \tTraining Loss: 5.556493\n",
            "Epoch is 274 \tTraining Loss: 5.303769\n",
            "Epoch is 275 \tTraining Loss: 5.572844\n",
            "Epoch is 276 \tTraining Loss: 5.395286\n",
            "Epoch is 277 \tTraining Loss: 5.252395\n",
            "Epoch is 278 \tTraining Loss: 5.292099\n",
            "Epoch is 279 \tTraining Loss: 5.455953\n",
            "Epoch is 280 \tTraining Loss: 5.447594\n",
            "Epoch is 281 \tTraining Loss: 5.271546\n",
            "Epoch is 282 \tTraining Loss: 5.237199\n",
            "Epoch is 283 \tTraining Loss: 5.342871\n",
            "Epoch is 284 \tTraining Loss: 5.272883\n",
            "Epoch is 285 \tTraining Loss: 5.292492\n",
            "Epoch is 286 \tTraining Loss: 5.370966\n",
            "Epoch is 287 \tTraining Loss: 5.194274\n",
            "Epoch is 288 \tTraining Loss: 5.219242\n",
            "Epoch is 289 \tTraining Loss: 5.205093\n",
            "Epoch is 290 \tTraining Loss: 5.374964\n",
            "Epoch is 291 \tTraining Loss: 5.040493\n",
            "Epoch is 292 \tTraining Loss: 5.230875\n",
            "Epoch is 293 \tTraining Loss: 5.224915\n",
            "Epoch is 294 \tTraining Loss: 5.115061\n",
            "Epoch is 295 \tTraining Loss: 5.162568\n",
            "Epoch is 296 \tTraining Loss: 5.014178\n",
            "Epoch is 297 \tTraining Loss: 5.154946\n",
            "Epoch is 298 \tTraining Loss: 5.190755\n",
            "Epoch is 299 \tTraining Loss: 5.249169\n",
            "Epoch is 300 \tTraining Loss: 5.225212\n",
            "Epoch is 301 \tTraining Loss: 5.151627\n",
            "Epoch is 302 \tTraining Loss: 5.328164\n",
            "Epoch is 303 \tTraining Loss: 5.091292\n",
            "Epoch is 304 \tTraining Loss: 5.137150\n",
            "Epoch is 305 \tTraining Loss: 5.019441\n",
            "Epoch is 306 \tTraining Loss: 5.187652\n",
            "Epoch is 307 \tTraining Loss: 5.141225\n",
            "Epoch is 308 \tTraining Loss: 5.209799\n",
            "Epoch is 309 \tTraining Loss: 5.176494\n",
            "Epoch is 310 \tTraining Loss: 5.071248\n",
            "Epoch is 311 \tTraining Loss: 4.980891\n",
            "Epoch is 312 \tTraining Loss: 4.915731\n",
            "Epoch is 313 \tTraining Loss: 5.250588\n",
            "Epoch is 314 \tTraining Loss: 5.130232\n",
            "Epoch is 315 \tTraining Loss: 4.984936\n",
            "Epoch is 316 \tTraining Loss: 4.978126\n",
            "Epoch is 317 \tTraining Loss: 4.885595\n",
            "Epoch is 318 \tTraining Loss: 5.000464\n",
            "Epoch is 319 \tTraining Loss: 4.982236\n",
            "Epoch is 320 \tTraining Loss: 5.054424\n",
            "Epoch is 321 \tTraining Loss: 4.766581\n",
            "Epoch is 322 \tTraining Loss: 4.894758\n",
            "Epoch is 323 \tTraining Loss: 4.963530\n",
            "Epoch is 324 \tTraining Loss: 4.875973\n",
            "Epoch is 325 \tTraining Loss: 4.992116\n",
            "Epoch is 326 \tTraining Loss: 4.868453\n",
            "Epoch is 327 \tTraining Loss: 5.001762\n",
            "Epoch is 328 \tTraining Loss: 5.014550\n",
            "Epoch is 329 \tTraining Loss: 4.797526\n",
            "Epoch is 330 \tTraining Loss: 4.877067\n",
            "Epoch is 331 \tTraining Loss: 4.758831\n",
            "Epoch is 332 \tTraining Loss: 4.925755\n",
            "Epoch is 333 \tTraining Loss: 4.900750\n",
            "Epoch is 334 \tTraining Loss: 5.001501\n",
            "Epoch is 335 \tTraining Loss: 4.723759\n",
            "Epoch is 336 \tTraining Loss: 4.876757\n",
            "Epoch is 337 \tTraining Loss: 4.933104\n",
            "Epoch is 338 \tTraining Loss: 4.587909\n",
            "Epoch is 339 \tTraining Loss: 4.757524\n",
            "Epoch is 340 \tTraining Loss: 4.835319\n",
            "Epoch is 341 \tTraining Loss: 4.719258\n",
            "Epoch is 342 \tTraining Loss: 4.583138\n",
            "Epoch is 343 \tTraining Loss: 4.730331\n",
            "Epoch is 344 \tTraining Loss: 4.704608\n",
            "Epoch is 345 \tTraining Loss: 4.651653\n",
            "Epoch is 346 \tTraining Loss: 4.802215\n",
            "Epoch is 347 \tTraining Loss: 4.712360\n",
            "Epoch is 348 \tTraining Loss: 4.746171\n",
            "Epoch is 349 \tTraining Loss: 4.664242\n",
            "Epoch is 350 \tTraining Loss: 4.796715\n",
            "Epoch is 351 \tTraining Loss: 4.726113\n",
            "Epoch is 352 \tTraining Loss: 4.548117\n",
            "Epoch is 353 \tTraining Loss: 4.536789\n",
            "Epoch is 354 \tTraining Loss: 4.693050\n",
            "Epoch is 355 \tTraining Loss: 4.609129\n",
            "Epoch is 356 \tTraining Loss: 4.845519\n",
            "Epoch is 357 \tTraining Loss: 4.408947\n",
            "Epoch is 358 \tTraining Loss: 4.532300\n",
            "Epoch is 359 \tTraining Loss: 4.764087\n",
            "Epoch is 360 \tTraining Loss: 4.508705\n",
            "Epoch is 361 \tTraining Loss: 4.534484\n",
            "Epoch is 362 \tTraining Loss: 4.608644\n",
            "Epoch is 363 \tTraining Loss: 4.701287\n",
            "Epoch is 364 \tTraining Loss: 4.581254\n",
            "Epoch is 365 \tTraining Loss: 4.566639\n",
            "Epoch is 366 \tTraining Loss: 4.755498\n",
            "Epoch is 367 \tTraining Loss: 4.328038\n",
            "Epoch is 368 \tTraining Loss: 4.629524\n",
            "Epoch is 369 \tTraining Loss: 4.646838\n",
            "Epoch is 370 \tTraining Loss: 4.468231\n",
            "Epoch is 371 \tTraining Loss: 4.601423\n",
            "Epoch is 372 \tTraining Loss: 4.481941\n",
            "Epoch is 373 \tTraining Loss: 4.523697\n",
            "Epoch is 374 \tTraining Loss: 4.520882\n",
            "Epoch is 375 \tTraining Loss: 4.528810\n",
            "Epoch is 376 \tTraining Loss: 4.525759\n",
            "Epoch is 377 \tTraining Loss: 4.627700\n",
            "Epoch is 378 \tTraining Loss: 4.495230\n",
            "Epoch is 379 \tTraining Loss: 4.438918\n",
            "Epoch is 380 \tTraining Loss: 4.536517\n",
            "Epoch is 381 \tTraining Loss: 4.539186\n",
            "Epoch is 382 \tTraining Loss: 4.504810\n",
            "Epoch is 383 \tTraining Loss: 4.425016\n",
            "Epoch is 384 \tTraining Loss: 4.481031\n",
            "Epoch is 385 \tTraining Loss: 4.431397\n",
            "Epoch is 386 \tTraining Loss: 4.393324\n",
            "Epoch is 387 \tTraining Loss: 4.461127\n",
            "Epoch is 388 \tTraining Loss: 4.513641\n",
            "Epoch is 389 \tTraining Loss: 4.529690\n",
            "Epoch is 390 \tTraining Loss: 4.542902\n",
            "Epoch is 391 \tTraining Loss: 4.495214\n",
            "Epoch is 392 \tTraining Loss: 4.466813\n",
            "Epoch is 393 \tTraining Loss: 4.363523\n",
            "Epoch is 394 \tTraining Loss: 4.433272\n",
            "Epoch is 395 \tTraining Loss: 4.367034\n",
            "Epoch is 396 \tTraining Loss: 4.382518\n",
            "Epoch is 397 \tTraining Loss: 4.403578\n",
            "Epoch is 398 \tTraining Loss: 4.349583\n",
            "Epoch is 399 \tTraining Loss: 4.356544\n",
            "Epoch is 400 \tTraining Loss: 4.267319\n",
            "Epoch is 401 \tTraining Loss: 4.309929\n",
            "Epoch is 402 \tTraining Loss: 4.457984\n",
            "Epoch is 403 \tTraining Loss: 4.288895\n",
            "Epoch is 404 \tTraining Loss: 4.304539\n",
            "Epoch is 405 \tTraining Loss: 4.374651\n",
            "Epoch is 406 \tTraining Loss: 4.425658\n",
            "Epoch is 407 \tTraining Loss: 4.479721\n",
            "Epoch is 408 \tTraining Loss: 4.245048\n",
            "Epoch is 409 \tTraining Loss: 4.308903\n",
            "Epoch is 410 \tTraining Loss: 4.241619\n",
            "Epoch is 411 \tTraining Loss: 4.350237\n",
            "Epoch is 412 \tTraining Loss: 4.437594\n",
            "Epoch is 413 \tTraining Loss: 4.343660\n",
            "Epoch is 414 \tTraining Loss: 4.179497\n",
            "Epoch is 415 \tTraining Loss: 4.424708\n",
            "Epoch is 416 \tTraining Loss: 4.101376\n",
            "Epoch is 417 \tTraining Loss: 4.194584\n",
            "Epoch is 418 \tTraining Loss: 4.263295\n",
            "Epoch is 419 \tTraining Loss: 4.359499\n",
            "Epoch is 420 \tTraining Loss: 4.184675\n",
            "Epoch is 421 \tTraining Loss: 4.180674\n",
            "Epoch is 422 \tTraining Loss: 4.180836\n",
            "Epoch is 423 \tTraining Loss: 4.070334\n",
            "Epoch is 424 \tTraining Loss: 4.292218\n",
            "Epoch is 425 \tTraining Loss: 4.311646\n",
            "Epoch is 426 \tTraining Loss: 4.194088\n",
            "Epoch is 427 \tTraining Loss: 4.331338\n",
            "Epoch is 428 \tTraining Loss: 4.088247\n",
            "Epoch is 429 \tTraining Loss: 4.148751\n",
            "Epoch is 430 \tTraining Loss: 4.266558\n",
            "Epoch is 431 \tTraining Loss: 4.155813\n",
            "Epoch is 432 \tTraining Loss: 4.210535\n",
            "Epoch is 433 \tTraining Loss: 4.262996\n",
            "Epoch is 434 \tTraining Loss: 4.195868\n",
            "Epoch is 435 \tTraining Loss: 4.344758\n",
            "Epoch is 436 \tTraining Loss: 4.333665\n",
            "Epoch is 437 \tTraining Loss: 4.031883\n",
            "Epoch is 438 \tTraining Loss: 4.269678\n",
            "Epoch is 439 \tTraining Loss: 4.350455\n",
            "Epoch is 440 \tTraining Loss: 4.186019\n",
            "Epoch is 441 \tTraining Loss: 4.233928\n",
            "Epoch is 442 \tTraining Loss: 4.077935\n",
            "Epoch is 443 \tTraining Loss: 4.165014\n",
            "Epoch is 444 \tTraining Loss: 4.184864\n",
            "Epoch is 445 \tTraining Loss: 4.315452\n",
            "Epoch is 446 \tTraining Loss: 4.046116\n",
            "Epoch is 447 \tTraining Loss: 4.193178\n",
            "Epoch is 448 \tTraining Loss: 4.075637\n",
            "Epoch is 449 \tTraining Loss: 4.224677\n",
            "Epoch is 450 \tTraining Loss: 4.196859\n",
            "Epoch is 451 \tTraining Loss: 4.098762\n",
            "Epoch is 452 \tTraining Loss: 4.042693\n",
            "Epoch is 453 \tTraining Loss: 4.049198\n",
            "Epoch is 454 \tTraining Loss: 4.176030\n",
            "Epoch is 455 \tTraining Loss: 4.250796\n",
            "Epoch is 456 \tTraining Loss: 4.191833\n",
            "Epoch is 457 \tTraining Loss: 4.322987\n",
            "Epoch is 458 \tTraining Loss: 4.148076\n",
            "Epoch is 459 \tTraining Loss: 4.017753\n",
            "Epoch is 460 \tTraining Loss: 4.123583\n",
            "Epoch is 461 \tTraining Loss: 3.978996\n",
            "Epoch is 462 \tTraining Loss: 4.088239\n",
            "Epoch is 463 \tTraining Loss: 3.904884\n",
            "Epoch is 464 \tTraining Loss: 3.958825\n",
            "Epoch is 465 \tTraining Loss: 4.159580\n",
            "Epoch is 466 \tTraining Loss: 4.320634\n",
            "Epoch is 467 \tTraining Loss: 4.286703\n",
            "Epoch is 468 \tTraining Loss: 4.049854\n",
            "Epoch is 469 \tTraining Loss: 4.068043\n",
            "Epoch is 470 \tTraining Loss: 4.073878\n",
            "Epoch is 471 \tTraining Loss: 4.164821\n",
            "Epoch is 472 \tTraining Loss: 3.903865\n",
            "Epoch is 473 \tTraining Loss: 4.043177\n",
            "Epoch is 474 \tTraining Loss: 3.959226\n",
            "Epoch is 475 \tTraining Loss: 4.096347\n",
            "Epoch is 476 \tTraining Loss: 4.164219\n",
            "Epoch is 477 \tTraining Loss: 4.288383\n",
            "Epoch is 478 \tTraining Loss: 4.047959\n",
            "Epoch is 479 \tTraining Loss: 3.947505\n",
            "Epoch is 480 \tTraining Loss: 3.950951\n",
            "Epoch is 481 \tTraining Loss: 4.003861\n",
            "Epoch is 482 \tTraining Loss: 4.097433\n",
            "Epoch is 483 \tTraining Loss: 4.211785\n",
            "Epoch is 484 \tTraining Loss: 4.258494\n",
            "Epoch is 485 \tTraining Loss: 4.062867\n",
            "Epoch is 486 \tTraining Loss: 3.908322\n",
            "Epoch is 487 \tTraining Loss: 3.942537\n",
            "Epoch is 488 \tTraining Loss: 3.946039\n",
            "Epoch is 489 \tTraining Loss: 4.007914\n",
            "Epoch is 490 \tTraining Loss: 4.005878\n",
            "Epoch is 491 \tTraining Loss: 3.866178\n",
            "Epoch is 492 \tTraining Loss: 4.023035\n",
            "Epoch is 493 \tTraining Loss: 3.956075\n",
            "Epoch is 494 \tTraining Loss: 3.854619\n",
            "Epoch is 495 \tTraining Loss: 3.987298\n",
            "Epoch is 496 \tTraining Loss: 4.036185\n",
            "Epoch is 497 \tTraining Loss: 4.012811\n",
            "Epoch is 498 \tTraining Loss: 3.869279\n",
            "Epoch is 499 \tTraining Loss: 4.031936\n",
            "Epoch is 500 \tTraining Loss: 4.075684\n",
            "mae= 7.381602\n"
          ]
        }
      ],
      "source": [
        "#匯入通用函式庫\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "step = 30 # 時序步數\n",
        "\n",
        "train_temp = pd.read_csv('./data/train.csv')\n",
        "test2_temp = pd.read_csv('./data/test.csv')\n",
        "\n",
        "#%%\n",
        "#依照訓練集數據長度建立空的資料結構\n",
        "n_data = train_temp.shape[0]\n",
        "time_step = step\n",
        "data = np.zeros((n_data,12,time_step),dtype = 'float32')\n",
        "\n",
        "row_start = 0\n",
        "row_end = 0\n",
        "tar_cols = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11', 'F12']\n",
        "#依照設定的步數處理數據\n",
        "row_end = len(train_temp)\n",
        "for j in range(time_step):\n",
        "    zero_cols = time_step - j - 1\n",
        "    train = pd.DataFrame([[0] * len(tar_cols)]*zero_cols,columns = tar_cols)\n",
        "    if zero_cols == 0:\n",
        "        train = train.append(train_temp[tar_cols][:])\n",
        "    else:\n",
        "        train = train.append(train_temp[tar_cols][:-zero_cols])\n",
        "    data[row_start:row_end,:,j] = train\n",
        "\n",
        "tar_cols = ['O']\n",
        "data_y = np.zeros((n_data,1),dtype = 'float32')\n",
        "row_start = 0\n",
        "row_end = len(train_temp)\n",
        "data_y[row_start:row_end,:] = train_temp[tar_cols]\n",
        "\n",
        "#%%\n",
        "#依照測試集數據長度建立空的資料結構\n",
        "n_data = test2_temp.shape[0]\n",
        "tar_cols = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11', 'F12']\n",
        "\n",
        "time_step = step\n",
        "test2_data = np.zeros((n_data,12,time_step),dtype = 'float32')\n",
        "row_start = 0\n",
        "row_end = 0\n",
        "#依照設定的步數處理數據\n",
        "row_end = len(test2_temp)\n",
        "for j in range(time_step):\n",
        "    zero_cols = time_step - j - 1\n",
        "    test = pd.DataFrame([[0] * len(tar_cols)]*zero_cols,columns = tar_cols)\n",
        "    if zero_cols == 0:\n",
        "        test = test.append(test2_temp[tar_cols][:])\n",
        "    else:\n",
        "        test = test.append(test2_temp[tar_cols][:-zero_cols])\n",
        "    test2_data[row_start:row_end,:,j] = test\n",
        "\n",
        "tar_cols = ['O']\n",
        "test2_y = np.zeros((n_data,1),dtype = 'float32')\n",
        "row_start = 0\n",
        "row_end = len(test2_temp)\n",
        "test2_y[row_start:row_end,:] = test2_temp[tar_cols]\n",
        "\n",
        "#%%\n",
        "#建立數據集張量與定義網路\n",
        "train_data = torch.from_numpy(data)\n",
        "train_y = torch.from_numpy(data_y)\n",
        "torch_dataset = Data.TensorDataset(train_data,  train_y)\n",
        "\n",
        "\n",
        "test2_data = torch.from_numpy(test2_data)\n",
        "test2_y = torch.from_numpy(test2_y)\n",
        "\n",
        "train_loader = Data.DataLoader(\n",
        "    dataset=torch_dataset,     \n",
        "    batch_size=1024,      \n",
        "    shuffle=True,               \n",
        "    num_workers=0,             \n",
        ")\n",
        "#%%\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #input_size = 特徵量\n",
        "        #hidden_size = 節點數\n",
        "        #num_layers = 隱藏層數量\n",
        "        self.rnn = nn.LSTM(input_size = 12, hidden_size = 16,num_layers = 1,batch_first  = False, bidirectional  = True)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(16, 16)\n",
        "        \n",
        "        self.bn2 = nn.BatchNorm1d(16)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        \n",
        "        self.bn3 = nn.BatchNorm1d(16)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(16, 16)\n",
        "        \n",
        "        self.bn4 = nn.BatchNorm1d(16)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(16, 16)\n",
        "        \n",
        "        self.bn5 = nn.BatchNorm1d(16)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        \n",
        "        self.fc5 = nn.Linear(16, 1)\n",
        "    def forward(self, x):\n",
        "        outputs, (ht, ct) = self.rnn(x)\n",
        "        \n",
        "        x = ht[-1]\n",
        "        residual = x\n",
        "        \n",
        "        x = self.fc1(self.relu1(self.bn1(x)))\n",
        "        x = self.fc2(self.relu2(self.bn2(x)))\n",
        "        x += residual\n",
        "        \n",
        "        residual = x\n",
        "        x = self.fc3(self.relu3(self.bn3(x)))\n",
        "        x = self.fc4(self.relu4(self.bn4(x)))\n",
        "        x += residual\n",
        "        \n",
        "        x = x = self.fc5(self.relu5(self.bn5(x)))\n",
        "        return x\n",
        "\n",
        "#%%\n",
        "#訓練\n",
        "net = Net()\n",
        "epoch_iter = 500 #1000\n",
        "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "learning_rate = 1e-4\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters() ,lr=learning_rate,weight_decay = 1)\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=list(range(epoch_iter,epoch_iter,12)), gamma=0.1)\n",
        "\n",
        "pths_path = './model/'\n",
        "\n",
        "for epoch in range(epoch_iter):\n",
        "    net.train()\n",
        "    epoch_loss = 0\n",
        "    val_loss = 0\n",
        "    for i, (x,y) in enumerate(train_loader):\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        x = x.permute(2,0,1)\n",
        "        pred_y = net(x)\n",
        "        loss1 = loss_fn(y[:,0], pred_y[:,0])\n",
        "        loss = loss1\n",
        "        epoch_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    net.eval()\n",
        "    train_loss = epoch_loss/((i+1)*2)\n",
        "    print('Epoch is {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "state_dict = net.state_dict()\n",
        "torch.save(state_dict,os.path.join(pths_path, 'model_step'+str(step)+'.pth'))\n",
        "\n",
        "#%%\n",
        "#測試\n",
        "net = Net()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "\n",
        "pths_path = './model/'\n",
        "net.load_state_dict(torch.load(os.path.join(pths_path, 'model_step'+str(step)+'.pth')))\n",
        "net.eval()\n",
        "\n",
        "x = test2_data.to(device)\n",
        "x = x.permute(2,0,1)\n",
        "pred_y = net(x)\n",
        "pred_y = pred_y.to('cpu')\n",
        "\n",
        "loss_fn(pred_y,test2_y)\n",
        "loss_fn(pred_y[:,0],test2_y[:,0])\n",
        "\n",
        "pred_y = pred_y.detach().numpy()\n",
        "\n",
        "mae = mean_absolute_error(test2_y, pred_y)\n",
        "print('mae=',mae)#8.607\n",
        "resultPath = './pred_result/'\n",
        "np.savetxt(resultPath+'testdata_Step'+str(step)+'.csv', pred_y, delimiter=',')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}